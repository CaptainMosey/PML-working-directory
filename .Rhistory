featurePlot(x=train[,c("roll_belt","pitch_belt","yaw_belt")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_arm","pitch_arm", "yaw_arm")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_dumbbell","pitch_dumbbell", "yaw_dumbbell")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_forearm","pitch_forearm", "yaw_forearm")], y=train$classe,plot="pairs")
for(i in 1:nrow(train)){
train$classn[i]<-utf8ToInt(as.character(train$classe[i]))-64
}
M<-abs(cor(train$classn,(train[,8:ncol(train)-2])))
diag(M)<-0
which(M>0.8, arr.ind=T)
print(M)
#M<-abs(cor(train))
#diag(M)<-0
#which(M>0.8, arr.ind=T)
#subset train
x<-colnames(train[,c(8:ncol(train)-2,ncol(train))])
train2<-train[,c(8:ncol(train)-2,ncol(train))]
colnames(train2)<-x
#predict w regression
lm1<-lm(classn~.,data=train2)
pred<-predict(lm1,train2)
qplot(classn,pred,data=train2)
#pml.training$class<-dummyVars(pml.training)
#try trees
x<-colnames(train[,c(8:ncol(train)-1)])
train2<-train[,c(8:ncol(train)-1)]
colnames(train2)<-x
#modFit<-train(classe~.,method="rpart2",data=train2,maxdepth=10)
modFit<-train(classe~.,method="rf",data=train2,prox=TRUE)
print(modFit)
getTree(modFit$finalModel,k=2)
print(predict(modFit,newdata=train2))
plot(train$classe,(predict(modFit,newdata=train2)))
#print(predict(modFit,newdata=trainL))
predTrain=predict(modFit,train2)
train2$predRight<-predTrain==train2$classe
print(table(predTrain,train2$classe))
test<-test[,-nearZeroVar(x=test,freqCut=95/5)]
train<-train[,-nearZeroVar(x=train,freqCut=95/5)]
test<-test[,-nearZeroVar(x=train,freqCut=95/5)]
test
train
test<-pml.training[-inTrain,]
test<-test[,-nearZeroVar(x=train,freqCut=95/5)]
summary(test)
#alot of skewed variables
#preObject<-preProcess(train,method=c("center","scale"))
#train<-predict(preObject,train)
#imputing
imputeObj=preProcess(test[,8:ncol(test)-2],method="knnImpute")
test[,8:ncol(test)-2]<-predict(imputeObj,test[,8:ncol(test)-2])
print(predict(modFit,newdata=test))
plot(test$classe,(predict(modFit,newdata=test)))
#print(predict(modFit,newdata=trainL))
predTest=predict(modFit,test)
test$predRight<-predTest==test$classe
print(table(predTest,test$classe))
test
View(test)
View(test)
inTrain<-createDataPartition(y=pml.training$classe,p=0.05,list=FALSE)
train<-pml.training[inTrain,]
test<-pml.training[-inTrain,]
test<-test[,-nearZeroVar(x=test,freqCut=95/5)]
summary(test)
imputeObj=preProcess(train[,8:ncol(train)-2],method="knnImpute")
imputeObj=preProcess(test[,8:ncol(test)-2],method="knnImpute")
test[,8:ncol(test)-2]<-predict(imputeObj,test[,8:ncol(test)-2])
print(predict(modFit,newdata=test))
plot(test$classe,(predict(modFit,newdata=test)))
predTest=predict(modFit,test)
test$predRight<-predTest==test$classe
print(table(predTest,test$classe))
library(ISLR)
library(caret)
library(Hmisc)
library(RANN)
library(ggplot2)
library(rattle)
set.seed(365)
#load from file
inTrain<-createDataPartition(y=pml.training$classe,p=0.05,list=FALSE)
train<-pml.training[inTrain,]
test<-pml.training[-inTrain,]
#inTrain<-createDataPartition(y=test$classe,p=0.05,list=FALSE)
#trainL<-test[inTrain,]
#test<-test[-inTrain]
#eliminate variables that dont cahange
removeColumns=nearZeroVar(x=train,freqCut=95/5)
train<-train[,-removeColumns]
#summary(train)
#alot of skewed variables
#preObject<-preProcess(train,method=c("center","scale"))
#train<-predict(preObject,train)
#imputing
imputeObj=preProcess(train[,8:ncol(train)-2],method="knnImpute")
train[,8:ncol(train)-2]<-predict(imputeObj,train[,8:ncol(train)-2])
featurePlot(x=train[,c("roll_belt","pitch_belt","yaw_belt")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_arm","pitch_arm", "yaw_arm")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_dumbbell","pitch_dumbbell", "yaw_dumbbell")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_forearm","pitch_forearm", "yaw_forearm")], y=train$classe,plot="pairs")
for(i in 1:nrow(train)){
train$classn[i]<-utf8ToInt(as.character(train$classe[i]))-64
}
M<-abs(cor(train$classn,(train[,8:ncol(train)-2])))
diag(M)<-0
which(M>0.8, arr.ind=T)
print(M)
#M<-abs(cor(train))
#diag(M)<-0
#which(M>0.8, arr.ind=T)
#subset train
x<-colnames(train[,c(8:ncol(train)-2,ncol(train))])
train2<-train[,c(8:ncol(train)-2,ncol(train))]
colnames(train2)<-x
#predict w regression
lm1<-lm(classn~.,data=train2)
pred<-predict(lm1,train2)
qplot(classn,pred,data=train2)
#pml.training$class<-dummyVars(pml.training)
#try trees
x<-colnames(train[,c(8:ncol(train)-1)])
train2<-train[,c(8:ncol(train)-1)]
colnames(train2)<-x
#modFit<-train(classe~.,method="rpart2",data=train2,maxdepth=10)
modFit<-train(classe~.,method="rf",data=train2,prox=TRUE)
print(modFit)
getTree(modFit$finalModel,k=2)
print(predict(modFit,newdata=train2))
plot(train$classe,(predict(modFit,newdata=train2)))
#print(predict(modFit,newdata=trainL))
predTrain=predict(modFit,train2)
train2$predRight<-predTrain==train2$classe
print(table(predTrain,train2$classe))
test<-test[,-removeColumns]
summary(test)
test[,8:ncol(test)-2]<-predict(imputeObj,test[,8:ncol(test)-2])
print(predict(modFit,newdata=test))
plot(test$classe,(predict(modFit,newdata=test)))
predTest=predict(modFit,test)
test$predRight<-predTest==test$classe
print(table(predTest,test$classe))
confidenceInterval = μ + c(-1, 1) * qt(quantile, df=n-1) * σ / sqrt(n)
n <- 9
μ <- 1100
σ <- 30
quantile = 0.975 # is 95% with 2.5% on both sides of the range
confidenceInterval = μ + c(-1, 1) * qt(quantile, df=n-1) * σ / sqrt(n)
confidenceInterval
n <- 9
averageDifference <- -2
quantile = 0.975 # is 95% with 2.5% on both sides of the range
ci_up = 0
σ = (ci_up - averageDifference * sqrt(n))/qt(quantile, df=n-1)
round(σ, 2)
?t.tes
?t.test
inTrain<-read.csv("pml-training.csv")
View(inTrain)
library(ISLR)
library(caret)
library(Hmisc)
library(RANN)
library(ggplot2)
library(rattle)
set.seed(365)
#load from file
inTrain<-read.csv("pml-training.csv")
View(inTrain)
#inTrain<-createDataPartition(y=pml.training$classe,p=0.6,list=FALSE)
train<-pml.training[inTrain,]
test<-pml.training[-inTrain,]
#inTrain<-createDataPartition(y=test$classe,p=0.05,list=FALSE)
#trainL<-test[inTrain,]
#test<-test[-inTrain]
#eliminate variables that dont cahange
removeColumns=nearZeroVar(x=train,freqCut=95/5)
train<-train[,-removeColumns]
#summary(train)
#alot of skewed variables
#preObject<-preProcess(train,method=c("center","scale"))
#train<-predict(preObject,train)
#imputing
imputeObj=preProcess(train[,8:ncol(train)-2],method="knnImpute")
train[,8:ncol(train)-2]<-predict(imputeObj,train[,8:ncol(train)-2])
featurePlot(x=train[,c("roll_belt","pitch_belt","yaw_belt")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_arm","pitch_arm", "yaw_arm")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_dumbbell","pitch_dumbbell", "yaw_dumbbell")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_forearm","pitch_forearm", "yaw_forearm")], y=train$classe,plot="pairs")
for(i in 1:nrow(train)){
train$classn[i]<-utf8ToInt(as.character(train$classe[i]))-64
}
M<-abs(cor(train$classn,(train[,8:ncol(train)-2])))
diag(M)<-0
which(M>0.8, arr.ind=T)
print(M)
#M<-abs(cor(train))
#diag(M)<-0
#which(M>0.8, arr.ind=T)
#subset train
x<-colnames(train[,c(8:ncol(train)-2,ncol(train))])
train2<-train[,c(8:ncol(train)-2,ncol(train))]
colnames(train2)<-x
#predict w regression
lm1<-lm(classn~.,data=train2)
pred<-predict(lm1,train2)
qplot(classn,pred,data=train2)
#pml.training$class<-dummyVars(pml.training)
#try trees
x<-colnames(train[,c(8:ncol(train)-1)])
train2<-train[,c(8:ncol(train)-1)]
colnames(train2)<-x
#modFit<-train(classe~.,method="rpart2",data=train2,maxdepth=10)
modFit<-train(classe~.,method="rf",data=train2,prox=TRUE)
print(modFit)
#getTree(modFit$finalModel,k=2)
#print(predict(modFit,newdata=train2))
plot(train$classe,(predict(modFit,newdata=train2)))
#print(predict(modFit,newdata=trainL))
predTrain=predict(modFit,train2)
train2$predRight<-predTrain==train2$classe
print(table(predTrain,train2$classe))
#Taketest data and do same
#eliminate variables that dont cahange
test<-test[,-removeColumns]
summary(test)
#alot of skewed variables
#preObject<-preProcess(train,method=c("center","scale"))
#train<-predict(preObject,train)
#imputing
#imputeObj=preProcess(test[,8:ncol(test)-2],method="knnImpute")
test[,8:ncol(test)-2]<-predict(imputeObj,test[,8:ncol(test)-2])
#print(predict(modFit,newdata=test))
plot(test$classe,(predict(modFit,newdata=test)))
#print(predict(modFit,newdata=trainL))
predTest=predict(modFit,test)
test$predRight<-predTest==test$classe
print(table(predTest,test$classe))
library(ISLR)
library(caret)
library(Hmisc)
library(RANN)
library(ggplot2)
library(rattle)
set.seed(365)
#load from file
inTrain<-read.csv("pml-training.csv")
View(inTrain)
inTrain<-createDataPartition(y=inTrain$classe,p=0.6,list=FALSE)
train<-pml.training[inTrain,]
test<-pml.training[-inTrain,]
#inTrain<-createDataPartition(y=test$classe,p=0.05,list=FALSE)
#trainL<-test[inTrain,]
#test<-test[-inTrain]
#eliminate variables that dont cahange
removeColumns=nearZeroVar(x=train,freqCut=95/5)
train<-train[,-removeColumns]
#summary(train)
#alot of skewed variables
#preObject<-preProcess(train,method=c("center","scale"))
#train<-predict(preObject,train)
#imputing
imputeObj=preProcess(train[,8:ncol(train)-2],method="knnImpute")
train[,8:ncol(train)-2]<-predict(imputeObj,train[,8:ncol(train)-2])
featurePlot(x=train[,c("roll_belt","pitch_belt","yaw_belt")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_arm","pitch_arm", "yaw_arm")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_dumbbell","pitch_dumbbell", "yaw_dumbbell")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_forearm","pitch_forearm", "yaw_forearm")], y=train$classe,plot="pairs")
for(i in 1:nrow(train)){
train$classn[i]<-utf8ToInt(as.character(train$classe[i]))-64
}
M<-abs(cor(train$classn,(train[,8:ncol(train)-2])))
diag(M)<-0
which(M>0.8, arr.ind=T)
print(M)
#M<-abs(cor(train))
#diag(M)<-0
#which(M>0.8, arr.ind=T)
#subset train
x<-colnames(train[,c(8:ncol(train)-2,ncol(train))])
train2<-train[,c(8:ncol(train)-2,ncol(train))]
colnames(train2)<-x
#predict w regression
lm1<-lm(classn~.,data=train2)
pred<-predict(lm1,train2)
qplot(classn,pred,data=train2)
#pml.training$class<-dummyVars(pml.training)
#try trees
x<-colnames(train[,c(8:ncol(train)-1)])
train2<-train[,c(8:ncol(train)-1)]
colnames(train2)<-x
#modFit<-train(classe~.,method="rpart2",data=train2,maxdepth=10)
modFit<-train(classe~.,method="rf",data=train2,prox=TRUE)
print(modFit)
#getTree(modFit$finalModel,k=2)
#print(predict(modFit,newdata=train2))
plot(train$classe,(predict(modFit,newdata=train2)))
#print(predict(modFit,newdata=trainL))
predTrain=predict(modFit,train2)
train2$predRight<-predTrain==train2$classe
print(table(predTrain,train2$classe))
#Taketest data and do same
#eliminate variables that dont cahange
test<-test[,-removeColumns]
summary(test)
#alot of skewed variables
#preObject<-preProcess(train,method=c("center","scale"))
#train<-predict(preObject,train)
#imputing
#imputeObj=preProcess(test[,8:ncol(test)-2],method="knnImpute")
test[,8:ncol(test)-2]<-predict(imputeObj,test[,8:ncol(test)-2])
#print(predict(modFit,newdata=test))
plot(test$classe,(predict(modFit,newdata=test)))
#print(predict(modFit,newdata=trainL))
predTest=predict(modFit,test)
test$predRight<-predTest==test$classe
print(table(predTest,test$classe))
library(ISLR)
library(caret)
library(Hmisc)
library(RANN)
library(ggplot2)
library(rattle)
set.seed(365)
#load from file
pml.training<-read.csv("pml-training.csv")
View(inTrain)
inTrain<-createDataPartition(y=pml.training$classe,p=0.6,list=FALSE)
train<-pml.training[inTrain,]
test<-pml.training[-inTrain,]
#inTrain<-createDataPartition(y=test$classe,p=0.05,list=FALSE)
#trainL<-test[inTrain,]
#test<-test[-inTrain]
#eliminate variables that dont cahange
removeColumns=nearZeroVar(x=train,freqCut=95/5)
train<-train[,-removeColumns]
#summary(train)
#alot of skewed variables
#preObject<-preProcess(train,method=c("center","scale"))
#train<-predict(preObject,train)
#imputing
imputeObj=preProcess(train[,8:ncol(train)-2],method="knnImpute")
train[,8:ncol(train)-2]<-predict(imputeObj,train[,8:ncol(train)-2])
featurePlot(x=train[,c("roll_belt","pitch_belt","yaw_belt")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_arm","pitch_arm", "yaw_arm")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_dumbbell","pitch_dumbbell", "yaw_dumbbell")], y=train$classe,plot="pairs")
featurePlot(x=train[,c("roll_forearm","pitch_forearm", "yaw_forearm")], y=train$classe,plot="pairs")
for(i in 1:nrow(train)){
train$classn[i]<-utf8ToInt(as.character(train$classe[i]))-64
}
M<-abs(cor(train$classn,(train[,8:ncol(train)-2])))
diag(M)<-0
which(M>0.8, arr.ind=T)
print(M)
#M<-abs(cor(train))
#diag(M)<-0
#which(M>0.8, arr.ind=T)
#subset train
x<-colnames(train[,c(8:ncol(train)-2,ncol(train))])
train2<-train[,c(8:ncol(train)-2,ncol(train))]
colnames(train2)<-x
#predict w regression
lm1<-lm(classn~.,data=train2)
pred<-predict(lm1,train2)
qplot(classn,pred,data=train2)
#pml.training$class<-dummyVars(pml.training)
#try trees
x<-colnames(train[,c(8:ncol(train)-1)])
train2<-train[,c(8:ncol(train)-1)]
colnames(train2)<-x
#modFit<-train(classe~.,method="rpart2",data=train2,maxdepth=10)
modFit<-train(classe~.,method="rf",data=train2,prox=TRUE)
print(modFit)
#getTree(modFit$finalModel,k=2)
#print(predict(modFit,newdata=train2))
plot(train$classe,(predict(modFit,newdata=train2)))
#print(predict(modFit,newdata=trainL))
predTrain=predict(modFit,train2)
train2$predRight<-predTrain==train2$classe
print(table(predTrain,train2$classe))
#Taketest data and do same
#eliminate variables that dont cahange
test<-test[,-removeColumns]
summary(test)
#alot of skewed variables
#preObject<-preProcess(train,method=c("center","scale"))
#train<-predict(preObject,train)
#imputing
#imputeObj=preProcess(test[,8:ncol(test)-2],method="knnImpute")
test[,8:ncol(test)-2]<-predict(imputeObj,test[,8:ncol(test)-2])
#print(predict(modFit,newdata=test))
plot(test$classe,(predict(modFit,newdata=test)))
#print(predict(modFit,newdata=trainL))
predTest=predict(modFit,test)
test$predRight<-predTest==test$classe
print(table(predTest,test$classe))
pml.testing <- read.csv("~/MOOC/Coursera - predictive machine learning/Practical Machine Learning project/pml-testing.csv")
View(pml.testing)
OSE<-1-(sum(test$predRight/nrow(test)))
print(paste("The out of sample error is ",toString(OSE),", with ",toString(sum(test$predRight)),"correct predicitions out of ",toString(nrow(test)," samples.")))
pml.testing<-pml.testing[,-removeColumns]
summary(pml.testing)
#Impute same- changed to -1 from -2 6:24pm 6/14/15
#test data has "problem ID column instead of classe"
pml.testing[,8:ncol(pml.testing)-1]<-predict(imputeObj,pml.testing[,8:ncol(pml.testing)-1])
pml.testing[,8:ncol(pml.testing)-2]<-predict(imputeObj,pml.testing[,8:ncol(pml.testing)-2])
predTesting=predict(modFit,pml.testing)
pml.testing$pred<-predTesting
a<-pml.testing$problem_id
a<-cbind(a,pml.testing$pred)
print(a)
testing$pred
pml.testing$pred
a<-pml.testing$problem_id
a<-cbind(a,as.character(pml.testing$pred))
print(a)
ans<-pml.testing$problem_id
ans<-cbind(a,as.character(pml.testing$pred))
print(ans)
ans<-pml.testing$problem_id
ans<-cbind(a,as.character(pml.testing$pred))
print(ans)
ans<-pml.testing$problem_id
ans<-cbind(ans,as.character(pml.testing$pred))
print(ans)
lmTrain=predict(lm1,train2)
train2$predRight<-lmTrain==train2$classe
print(table(lmTrain,train2$classe))
lmTrain<-predict(lm1,train2)
View(ans)
View(ans)
pred<-predict(lm1,train2)
pred
lmTrain<-pred
train2$predRight<-lmTrain==train2$classe
print(table(lmTrain,train2$classe))
test<-pml.training[-inTrain,]
train<-pml.training[inTrain,]
for(i in 1:nrow(train)){
train$classn[i]<-utf8ToInt(as.character(train$classe[i]))-64
}
x<-colnames(train[,c(8:ncol(train)-2,ncol(train))])
train2<-train[,c(8:ncol(train)-2,ncol(train))]
colnames(train2)<-x
#predict w regression
lm1<-lm(classn~.,data=train2)
pred<-predict(lm1,train2)
qplot(classn,pred,data=train2)
for(i in 1:nrow(test)){
test$classn[i]<-utf8ToInt(as.character(test$classe[i]))-64
}
x<-colnames(test[,c(8:ncol(test)-2,ncol(test))])
test2<-test[,c(8:ncol(test)-2,ncol(test))]
colnames(test2)<-x
lmTest=predict(modFit,test2)
test2$predRight<-lmTest==test2$classe
print(table(lmTest,test2$classe))
#out of sample error
OSE<-1-(sum(test$predRight/nrow(test)))
print(paste("The out of sample error is ",toString(OSE),", with ",toString(sum(test$predRight)),"correct predicitions out of ",toString(nrow(test)," samples.")))
colnames(test2)
colnames(train2)
lmTest
train<-pml.training[inTrain,]
test<-pml.training[-inTrain,]
for(i in 1:nrow(train)){
train$classn[i]<-utf8ToInt(as.character(train$classe[i]))-64
}
#x<-colnames(train[,c(8:ncol(train)-2,ncol(train))])
#train2<-train[,c(8:ncol(train)-2,ncol(train))]
x<-colnames(train[,c(8:ncol(train))])
train2<-train[,c(8:ncol(train))]
colnames(train2)<-x
#predict w regression
lm1<-lm(classn~.,data=train2)
pred<-predict(lm1,train2)
qplot(classn,pred,data=train2)
train<-pml.training[inTrain,]
test<-pml.training[-inTrain,]
removeColumns=nearZeroVar(x=train,freqCut=95/5)
train<-train[,-removeColumns]
#changed to -1 from -2, 6/14/15 6:04p
imputeObj=preProcess(train[,8:ncol(train)-2],method="knnImpute")
train[,8:ncol(train)-2]<-predict(imputeObj,train[,8:ncol(train)-2])
?t.test
?rnorm
new=rnorm(50,3,0.6)
old=(50,5,0.68)
old=rnorm(50,5,0.68)
t.test(new,old)
old=(10,5,0.68)
old=rnorm(10,5,0.68)
new=rnorm(10,3,0.6)
t.test(new,old)
?z.test
new=rnorm(1000,3,0.6)
old=rnorm(1000,5,0.68)
t.test(new,old)
?rf
?predict
?predict.rf
?train
new=rnorm(100,4,0.5)
old=rnorm(100,6,0.2)
t.test(new,old)
t.test(old,new)
diet=rnorm(9,3,1.5)
placebo=rnorm(9,1,1.8)
?t.test
t.test(diet,placebo,paired=T,conf.level=0.90)
diet=rnorm(9,-3,1.5)
t.test(diet,placebo,paired=T,conf.level=0.90)
